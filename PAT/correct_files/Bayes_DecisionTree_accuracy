#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri May  5 01:28:17 2017

@author: patrizio
"""



import nltk

import random


from Word2Vec import Word2Bigrams, Pos2Vec

def FeaturesExtractor (word):
    def nVowels (word):
        vowels='a e i o u'.split()
        nvowels = 0
        
        for i in range(len(word)):
            if word[i] in vowels:
                nvowels += 1 
        return nvowels

    f1 = word[-1]   # last character
    f2 = word[-2:] # last 2 char
    f3 = word[-3:]
    f4 = word[0]
    f5 = word[:2]
    f6 = word[:3]
    f7 = nVowels(word) #voc 
    f8 = len(word) # length
    f9 = f8 - f7  #conson
    
    return {'finale':f1,'finale-2':f2,'suffisso':f3,'iniziale':f4,
            'iniziale-2':f5,'prefisso':f6, 'nVowels':f7, 'nConson':f9,
            'len':f8}

trainDim = 0.8
w2v = Word2Bigrams()
p2v = Pos2Vec()

#words = p2v.words.keys()
words = [w for w in p2v.words.keys() if w2v._ValidateWord(w)]
# restricting set size
#words = words[:150000]

random.shuffle(words)
trainDim = int(trainDim*len(words))

train_set, test_set = words[:trainDim], words[trainDim:]

print ('datasets creation')
train_set = [(FeaturesExtractor(w), p2v.words[w]) for w in train_set]
test_set = [(FeaturesExtractor(w), p2v.words[w]) for w in test_set]
print ('Bayes training')
classifier = nltk.NaiveBayesClassifier.train(train_set)
print('Bayes accuracy', nltk.classify.accuracy(classifier, test_set))

#classifier.classify(gender_features('Neo'))
classifier.show_most_informative_features(150)

# decision tree
print ('decision tree')
classifier = nltk.DecisionTreeClassifier.train(train_set)
print ('decision tree accuracy:',nltk.classify.accuracy(classifier, test_set))

#classifier.classify(pos_features('cats'))
#print ('decision tree classifier pseudocode with depth = 4')
classifier.pseudocode(depth=4)


#print ('decision tree classifier pseudocode with depth = 15')
#print(classifier.pseudocode(depth=15))




# example linear regression from nltk
from nltk.classify.scikitlearn import SklearnClassifier
#from nltk.classify.util import names_demo, names_demo_features
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import BernoulliNB, MultinomialNB

#http://www.nltk.org/howto/classify.html

#http://www.nltk.org/_modules/nltk/classify/util.html#names_demo_features


def accuracy(classifier, gold):
    results = classifier.classify_many([fs for (fs, l) in gold])
    correct = [l == r for ((fs, l), r) in zip(gold, results)]
    if correct:
        return sum(correct) / len(correct)
    else:
        return 0
    
def My_classifier_linear_regression(trainer, features=FeaturesExtractor, data=None, trainDim=0.8):
    """
        data is a list with a pair (word, tag) for each items
    """
    # Construct a list of classified names, using the names corpus.
    trainDim = int(len(data)*trainDim)
    train = data[:trainDim]
    test = data[trainDim:]

    # Train up a classifier.
    print('Training classifier...')
    classifier = trainer( [(features(n), g) for (n, g) in train] )

    # Run the classifier on the test data.
    print('Testing classifier...')
    acc = accuracy(classifier, [(features(n), g) for (n, g) in test])
    print('Accuracy: %6.4f' % acc)

    # For classifiers that can find probabilities, show the log
    # likelihood and some sample probability distributions.
    try:
        test_featuresets = [features(n) for (n, g) in test]
        pdists = classifier.prob_classify_many(test_featuresets)
#        ll = [pdist.logprob(gold)
#              for ((name, gold), pdist) in zip(test, pdists)]
#        print('Avg. log likelihood: %6.4f' % (sum(ll) / len(test)))
#        print()
##==============================================================================
##         change this 
##==============================================================================
#        print('Unseen Names      P(Male)  P(Female)\n'+'-'*40)
#        for ((name, gender), pdist) in list(zip(test, pdists))[:5]:
#            if gender == 'male':
#                fmt = '  %-15s *%6.4f   %6.4f'
#            else:
#                fmt = '  %-15s  %6.4f  *%6.4f'
#            print(fmt % (name, pdist.prob('male'), pdist.prob('female')))
        return test, pdists
    except NotImplementedError:
        pass

    # Return the classifier
    return classifier

words = [w for w in p2v.words.keys() if w2v._ValidateWord(w)]
random.shuffle(words)
#words = words[:15000]

data = [(w, p2v.words[w]) for w in words]

for c in [1, 10, 100, 1000, 10000]:
    print ('Logistic Regression', c)
    test_svm, pdists_svm = My_classifier_linear_regression(
            SklearnClassifier(LogisticRegression(C=c)).train, 
            features=FeaturesExtractor, data=data)
        

print ('BernoulliNB binarize = False')
test_bern, pdists_bern = My_classifier_linear_regression(
        SklearnClassifier(BernoulliNB(binarize=False)).train, 
        features=FeaturesExtractor, data=data)


print ('BernoulliNB binarize = True')
test_bern, pdists_bern = My_classifier_linear_regression(
        SklearnClassifier(BernoulliNB(binarize=True)).train, 
        features=FeaturesExtractor, data=data)


print ('MultinomialNB')
test_multiNB, pdists_multiNB = My_classifier_linear_regression(
        SklearnClassifier(MultinomialNB()).train, 
        features=FeaturesExtractor, data=data)
    

