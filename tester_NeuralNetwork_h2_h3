#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Universit√† degli studi di Trento (Tn) - Italy
Center for Mind/Brain Sciences CIMeC
Language, Interaction and Computation Laboratory CLIC

@author: Patrizio Bellan
         patrizio.bellan@gmail.com
         patrizio.bellan@studenti.unitn.it

         github.com/patriziobellan86


        Francesco Mantegna
        fmantegna93@gmail.com
          
"""

from __future__ import print_function


import random
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

from Word2Vec import Word2Bigrams, Pos2Vec
import csv

w2v = Word2Bigrams()
p2v = Pos2Vec()

#==============================================================================
# Parameters
#==============================================================================
model_ext = ".ckpt"

trainDim = 0.7
testDim = 0.2
validDim = 0.1

# Training Parameters
learning_rate = 0.05 #0.001
training_epochs = 50 #80
batch_size = 50
display_step = 10

# Network Parameters
n_hidden_1 = 250#1000#50 # 1st layer number of features
n_hidden_2 = 125#250#500 # 2nd layer number of features
n_hidden_3 = 25#50#500 # 2nd layer number of features

n_input = len(w2v.bidict)
n_classes = len(p2v.posdict)

# optimazer params
beta1=0.9
beta2=0.999
epsilon=1e-02 #1e-08


def SaveResults (filename, data):
    outfile  = open('result_h2.csv', "a")
    writer = csv.writer(filename, delimiter=';', quotechar='"')
    # write headers
    writer.writerow(data)
    outfile.close()


class DataSet:
    def __init__(self, words):

        w2v = Word2Bigrams()
        p2v = Pos2Vec()
        
        self._index_in_epoch = 0
        
        self.input = [] # store the input vector
        self.output =[] # store the output vector
        self.word = []  # store the word
        self.pos = []   # store the pos of the word
        
        for w in words:
#            print ('dataset element: ',w)
            vec = w2v.Word2Vec(w)
            # check if it is a valid word
            if vec:
                self.input.append(vec)
                pos = p2v.words[w]
                self.output.append(p2v.Pos2Vec(pos))
                self.word.append(w)
                self.pos.append(pos)
                
        self.input = np.array(self.input)
        self.output = np.array(self.output)
    
    def reset_epoch (self):
        self._index_in_epoch = 0
        
    def next_batch(self, batch_size):
        """Return the next `batch_size` examples from this data set."""
        start = self._index_in_epoch
        self._index_in_epoch += batch_size
        end = self._index_in_epoch
        return self.input[start:end], self.output[start:end]

    def next_batch_extended(self, batch_size):
        """Return the next `batch_size` examples from this data set."""
        start = self._index_in_epoch
        self._index_in_epoch += batch_size
        end = self._index_in_epoch
        return self.input[start:end], self.output[start:end], self.word[start:end], self.pos[start:end]


# words for feeding the nn
words = list(p2v.words.keys())

# testing, only the first 5000 words
words = words[:5000]

random.shuffle(words)

# dataset dimension
trainDim = int(trainDim*len(words))   
testDim = int(testDim*len(words))
validDim = int(validDim*len(words))    
# words lists    
train = words[-trainDim:]    
test = words[:testDim]
validate = words[testDim:trainDim]    
# dataset creation    
train = DataSet(train)
test = DataSet(test)
validate = DataSet(validate)

# total batch
total_batch = int(len(train.word)/batch_size)

#==============================================================================
def TestingNNs():
    for learning_rate in [0.001, 0.05]:
        for training_epochs in [50, 100]:
            for batch_size in [50, 80]:
                for n_hidden_1 in [250, 500, 1000]:
                    for n_hidden_2 in [125, 250, 500]:
                        for n_hidden3 in [25, 50, 500]:
                            for epsilon in [1e-02, 1e-08]:
                                h2(learning_rate,training_epochs, 
                                   batch_size,n_hidden_1, n_hidden_2,n_hidden3,
                                   n_input,n_classes,beta1,beta2,epsilon,
                                   'h2')
                                h3(learning_rate,training_epochs, 
                                   batch_size,n_hidden_1, n_hidden_2,n_hidden3,
                                   n_input,n_classes,beta1,beta2,epsilon,
                                   'h3')
                                
#==============================================================================
# H2
#==============================================================================
                                        
def h2(learning_rate,training_epochs, batch_size,n_hidden_1, n_hidden_2,n_hidden3,
                n_input,n_classes,beta1,beta2,epsilon,
                filename):    
    
    # tf Graph input
    x = tf.placeholder("float", shape=(None, n_input))
    y = tf.placeholder("float", shape=(None, n_classes))
    
    # Create model
    def multilayer_perceptron(x, weights, biases):
        # Hidden layer with RELU activation
        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])
        layer_1 = tf.nn.relu(layer_1)
        # Hidden layer with RELU activation
        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])
        layer_2 = tf.nn.relu(layer_2)
        # Output layer with linear activation
        out_layer = tf.matmul(layer_2, weights['out']) + biases['out']
        
        return out_layer
    
    # Store layers weight & bias
    weights = {
        'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),
        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),
        'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))
    }
    biases = {
        'b1': tf.Variable(tf.random_normal([n_hidden_1])),
        'b2': tf.Variable(tf.random_normal([n_hidden_2])),
        'out': tf.Variable(tf.random_normal([n_classes]))
    }
    
    # Construct model
    pred = multilayer_perceptron(x, weights, biases)
    
    # Define loss and optimizer
    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)) # sparse_softmax_cross_entropy_with_logits
    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=beta1, beta2=beta2,epsilon=epsilon).minimize(cost)
    
    correct_pred = tf.equal(tf.argmax(y,1),tf.argmax(pred,1))
    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
    
    
    ## Launch the graph
    
    # Initializing the variables
    init = tf.global_variables_initializer()
    
    sess = tf.Session()    
    sess.run(init)
    
    
    totalLoss = 0
    totalAcc = 0
    # Training cycle
    lines=[]     # store lines for plot
    linesvalidate = []
    for epoch in range(training_epochs):
        print(epoch)
        dataplot = []
        avg_cost = 0.
        print('total batch', total_batch)
        # Loop over all batches
        for i in range(total_batch-1):
            batch_x, batch_y = train.next_batch(batch_size)
            # Run optimization op (backprop) and cost op (to get loss value)
    #        feed_dict={x: batch_x, y: batch_y}
            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})
            print ('c',c, 'c/tbatch',c / total_batch, 'avg_cost', avg_cost)
            # Compute average loss
            avg_cost += c / total_batch
            dataplot.append(avg_cost)
        
        # Display logs per epoch step
        if epoch % display_step == 0:
            print("Epoch:", '%04d' % (epoch+1), "cost=", \
                "{:.9f}".format(avg_cost))
        acc = accuracy.eval(feed_dict={x: batch_x, y: batch_y}, session=sess)
        linesvalidate.append(acc)         
        print('accuracy epoch', acc)    
    
        lines.append(dataplot)
    
        train.reset_epoch()
        
    totalLoss= avg_cost
    
    
    plt.plot(lines)
    plt.show()
    
    
    plt.plot(linesvalidate)
    plt.show()  
     
    print("Optimization Finished!")
    
    saver = tf.train.Saver()
    # Save model weights to disk
    save_path = saver.save(sess, 'model'+model_ext)
    print("Model saved")
    
    batch_x, batch_y = test.next_batch(len(test.word))
    feed_dict={x: batch_x, y: batch_y}
    acc =  accuracy.eval(feed_dict=feed_dict, session=sess)         
    print('accuracy',acc)    
    totalAcc = acc
    trainacc = sum(linesvalidate)/len(linesvalidate)
    print ('loss', totalLoss,'accuracy', totalAcc, 'training accuracy',trainacc)
    print ('training acc - acc: ', acc - trainacc)
    
    
    line = [learning_rate,training_epochs, batch_size,n_hidden_1, n_hidden_2,0,
                n_input,n_classes,beta1,beta2,epsilon,
                totalLoss,totalAcc,trainacc,acc-trainacc]
    SaveResults(filename+'.csv', line)
    

#==============================================================================
# H3
#==============================================================================

def h3 (learning_rate,training_epochs, batch_size,n_hidden_1, n_hidden_2,n_hidden3,
                n_input,n_classes,beta1,beta2,epsilon,
                filename):
    
    
    # tf Graph input
    x = tf.placeholder("float", shape=(None, n_input))
    y = tf.placeholder("float", shape=(None, n_classes))
    
    # Create model
    def multilayer_perceptron(x, weights, biases):
        # Hidden layer with RELU activation
        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])
        layer_1 = tf.nn.relu(layer_1)
        # Hidden layer with RELU activation
        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])
        layer_2 = tf.nn.relu(layer_2)
        
        layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])
        layer_3 = tf.nn.relu(layer_3)
        
        # Output layer with linear activation
        out_layer = tf.matmul(layer_3, weights['out']) + biases['out']
        
        return out_layer
    
    # Store layers weight & bias
    weights = {
        'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),
        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),
        'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),
        'out': tf.Variable(tf.random_normal([n_hidden_3, n_classes]))
    }
    biases = {
        'b1': tf.Variable(tf.random_normal([n_hidden_1])),
        'b2': tf.Variable(tf.random_normal([n_hidden_2])),
        'b3': tf.Variable(tf.random_normal([n_hidden_3])),
        'out': tf.Variable(tf.random_normal([n_classes]))
    }
    
    # Construct model
    pred = multilayer_perceptron(x, weights, biases)
    
    # Define loss and optimizer
    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)) # sparse_softmax_cross_entropy_with_logits
    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=beta1, beta2=beta2,epsilon=epsilon).minimize(cost)
    
    correct_pred = tf.equal(tf.argmax(y,1),tf.argmax(pred,1))
    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
    
    
    ## Launch the graph
    
    # Initializing the variables
    init = tf.global_variables_initializer()
    
    sess = tf.Session()    
    sess.run(init)
    
    
    totalLoss = 0
    totalAcc = 0
    # Training cycle
    lines=[]     # store lines for plot
    linesvalidate = []
    for epoch in range(training_epochs):
        print(epoch)
        dataplot = []
        avg_cost = 0.
        print('total batch', total_batch)
        # Loop over all batches
        for i in range(total_batch-1):
            batch_x, batch_y = train.next_batch(batch_size)
            # Run optimization op (backprop) and cost op (to get loss value)
    #        feed_dict={x: batch_x, y: batch_y}
            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})
            print ('c',c, 'c/tbatch',c / total_batch, 'avg_cost', avg_cost)
            # Compute average loss
            avg_cost += c / total_batch
            dataplot.append(avg_cost)
        
        # Display logs per epoch step
        if epoch % display_step == 0:
            print("Epoch:", '%04d' % (epoch+1), "cost=", \
                "{:.9f}".format(avg_cost))        
        acc = accuracy.eval(feed_dict={x: batch_x, y: batch_y}, session=sess)
        linesvalidate.append(acc)         
        print('accuracy epoch', acc)    
    
        lines.append(dataplot)
    
        train.reset_epoch()
        
    totalLoss= avg_cost
    
    
    plt.plot(lines)
    plt.show()
    
    
    plt.plot(linesvalidate)
    plt.show()  
     
    print("Optimization Finished!")
    
    saver = tf.train.Saver()
    # Save model weights to disk
    save_path = saver.save(sess, 'model'+model_ext)
    print("Model saved")
    
    # Test model    
    batch_x, batch_y = test.next_batch(len(test.word))
    feed_dict={x: batch_x, y: batch_y}
    acc =  accuracy.eval(feed_dict=feed_dict, session=sess)         
    print('accuracy',acc)    
    totalAcc = acc
    trainacc = sum(linesvalidate)/len(linesvalidate)
    print ('loss', totalLoss,'accuracy', totalAcc, 'training accuracy',trainacc)
    print ('training acc - acc: ', acc - trainacc)
    
    line = [learning_rate,training_epochs, batch_size,n_hidden_1, n_hidden_2,n_hidden_3,
            n_input,n_classes,beta1,beta2,epsilon,
            totalLoss,totalAcc,trainacc,acc-trainacc]

    SaveResults(filename+'.csv', line)